{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import date \n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv(\"demo_try.csv\", parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.loc[:,['Name','Date','Quantity','Item : Internal ID']]\n",
    "data_new.rename(columns={'Item : Internal ID': 'Product_Id', 'Name':'Customer_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_new.groupby(['Customer_Name', 'Product_Id', 'Date', 'Quantity']).size().reset_index()\n",
    "# result = result.count() #result['Customer_Name'].value_counts()\n",
    "result.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_customer = set(data_new['Customer_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_customer_details(customer_name, result):\n",
    "    data_final = result[result['Customer_Name'] == customer_name]\n",
    "    return data_final\n",
    "\n",
    "def prod_wise_customer(prod_wise_cus, gr_wise_pr):\n",
    "    final_ = prod_wise_cus.get_group(gr_wise_pr)\n",
    "    final_ = final_.loc[:,['Date','Quantity']]\n",
    "    final_[\"Date\"]= pd.to_datetime(final_[\"Date\"], dayfirst=True)\n",
    "    final_.set_index('Date', inplace=True)\n",
    "    final_.fillna(0,inplace=True)\n",
    "    final_ =final_['Quantity'].resample('D').sum()\n",
    "    print(final_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for customer in all_customer:\n",
    "    customer_info = lock_customer_details(customer, result)\n",
    "    prod_wise_cus = customer_info.groupby(['Product_Id'])\n",
    "    for gr_wise_pr in prod_wise_cus.groups.keys():\n",
    "        final_df = prod_wise_customer(prod_wise_cus, gr_wise_pr)\n",
    "        print(final_df)\n",
    "        break\n",
    "    # print(customer_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simple_Exponential_Smooting(train_data, n_period):\n",
    "    model_simple_es = SimpleExpSmoothing(train_data).fit()\n",
    "    predictions_simple_es = model_simple_es.forecast(n_period)\n",
    "    test_pred_ses = predictions_simple_es[-n_period:]\n",
    "    predictions_ses_final = test_pred_ses.mask(test_pred_ses<0,0).apply(np.ceil)\n",
    "    return predictions_ses_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Simple_Exponential_Smooting(final_df, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Markov Model initialization\n",
    "model = hmm.MultinomialHMM(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[5, 9, 14, 55], [66, 9, 15, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_purchase = model.predict([[66, 9, 15, 5]])  # Use February data as input\n",
    "print(f\"Predicted next purchase: Product {next_purchase[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import date \n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv(\"demo_try.csv\", parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.loc[:,['Customer Name','Date','Quantity','Item : Internal ID']]\n",
    "data_new.rename(columns={'Item : Internal ID': 'Product_Id', 'Customer Name':'Customer_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_new.groupby(['Customer_Name', 'Product_Id', 'Date', 'Quantity']).size().reset_index()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_customer = set(data_new['Customer_Name'])\n",
    "all_product = set(result['Product_Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_attribute(resample_type):\n",
    "    if resample_type == \"M\":\n",
    "        thresold_nonzero = 15\n",
    "        test_period = 8\n",
    "    elif resample_type == \"Q\":\n",
    "        thresold_nonzero = 20\n",
    "        test_period = 3\n",
    "    elif resample_type == \"Y\":\n",
    "        thresold_nonzero = 40\n",
    "        test_period = 1\n",
    "    return thresold_nonzero, test_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Double_Exponential_Smoothing(train_data, test_data, n_period, flag, resample_type):\n",
    "    double_model_es = ExponentialSmoothing(train_data, trend='add').fit()\n",
    "    predictions_double_es = double_model_es.forecast(n_period)\n",
    "    test_pred_des = predictions_double_es[-n_period:]\n",
    "    predictions_des_final = test_pred_des.mask(test_pred_des<0,0).apply(np.ceil)\n",
    "    predictions_des_final_raw = test_pred_des.mask(test_pred_des<0,0)\n",
    "    if flag == 1:\n",
    "        mae_des =np.round(np.mean(np.abs(test_data - predictions_des_final)),2)\n",
    "        return \"Double Exponential Smooting\", mae_des\n",
    "    else:\n",
    "        if resample_type == \"M\":\n",
    "            std_des = np.std(train_data[-24:])    #predictions_des_final) -24\n",
    "            if std_des > 0:\n",
    "                ci_des = 1.28 * std_des           ### 80% Confidence Level\n",
    "            else:\n",
    "                ci_des = (predictions_des_final * .2)\n",
    "            weight_list = [4,4,4,5,5,8,8,10,10,13,14,15] #= 100\n",
    "            # weight_list = [12,18,20,25,25]\n",
    "            # weight_list = [10,20,35,35]\n",
    "            historic_value = list(train_data[-12:])\n",
    "            weighted_avg = np.round(np.average( historic_value, weights = weight_list), 2)\n",
    "            weighted_avg = np.maximum(np.ceil(weighted_avg), 0) #np.ceil(weighted_avg)\n",
    "            dict_sheet = {}\n",
    "            conf_int_des=pd.DataFrame({\n",
    "                                       'lower': np.floor(predictions_des_final - ci_des) ,\n",
    "                                       'upper':np.ceil(predictions_des_final + ci_des)}, \n",
    "                                        index=predictions_des_final.index)\n",
    "            \n",
    "            if (int(predictions_des_final[0]) > int(weighted_avg * .7)) and (int(predictions_des_final[0]) <  int(weighted_avg * 1.4)):    \n",
    "                dict_sheet['Forecasted_value'] = predictions_des_final\n",
    "                dict_sheet['Lower'] = conf_int_des.lower.mask(conf_int_des.lower<0,0)\n",
    "                dict_sheet['Upper'] = conf_int_des.upper.mask(conf_int_des.upper<0,0)\n",
    "                return dict_sheet\n",
    "            else:\n",
    "                forecasted_avg = np.ceil(weighted_avg)\n",
    "                forecasted_min = np.floor(forecasted_avg - (forecasted_avg * .2))\n",
    "                forecasted_max = np.ceil(forecasted_avg + (forecasted_avg * .25))\n",
    "\n",
    "                predictions_des_final[predictions_des_final.index[0]] = forecasted_avg\n",
    "                conf_int_des.lower[predictions_des_final.index[0]] = forecasted_min\n",
    "                conf_int_des.upper[predictions_des_final.index[0]] = forecasted_max\n",
    "\n",
    "                dict_sheet['Forecasted_value'] = predictions_des_final.mask(predictions_des_final<0,0)\n",
    "                dict_sheet['Lower'] = conf_int_des.lower.mask(conf_int_des.lower<0,0)\n",
    "                dict_sheet['Upper'] = conf_int_des.upper.mask(conf_int_des.upper<0,0)\n",
    "                return dict_sheet\n",
    "        else:\n",
    "            std_des_raw = np.std(train_data[-24:])   #predictions_des_final_raw)\n",
    "            if std_des_raw > 0:\n",
    "                ci_des_raw = 1.28 * std_des_raw      ### 80% Confidence Level\n",
    "            else:\n",
    "                ci_des_raw = (predictions_des_final_raw * .2)\n",
    "            conf_int_des=pd.DataFrame({\n",
    "                                      'lower': np.floor(predictions_des_final_raw - ci_des_raw),\n",
    "                                      'upper':np.ceil(predictions_des_final_raw + ci_des_raw)},\n",
    "                                       index=predictions_des_final_raw.index)\n",
    "            dict_sheet = {}\n",
    "            dict_sheet['Forecasted_value'] = np.round(predictions_des_final_raw)\n",
    "            dict_sheet['Lower'] = conf_int_des.lower.mask(conf_int_des.lower<0,0)\n",
    "            dict_sheet['Upper'] = conf_int_des.upper.mask(conf_int_des.upper<0,0)\n",
    "            return dict_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMA(train_data, test_data, n_period, flag, resample_type, check_period):\n",
    "  if resample_type == \"Q\": \n",
    "        simple_diff = True\n",
    "        check_period = 3\n",
    "  else: \n",
    "        simple_diff = False\n",
    "        # check_period = int(np.round(len(train_data)*.1))\n",
    "        check_period = check_period\n",
    "  p = d = q = range(1, 3)\n",
    "  pdq_a = list(itertools.product(p, d, q))\n",
    "  minimum_mae = math.inf\n",
    "  for param in pdq_a:\n",
    "        modd = sm.tsa.statespace.SARIMAX(train_data, order = param, simple_differencing=simple_diff)                                  \n",
    "        try:\n",
    "            resultsss = modd.fit(disp=False)\n",
    "            predict_first = resultsss.get_prediction(end = modd.nobs-1)\n",
    "            pred_arima_mean = np.round(predict_first.predicted_mean)\n",
    "            pred_arima_masked = pred_arima_mean.mask(pred_arima_mean<=0,0)\n",
    "            # print(\"pred_arima_mean\", param, pred_arima_masked[-12:], train_data[-12:])\n",
    "            mae_arima_first =np.round(np.mean(np.abs(train_data[-check_period:] - pred_arima_masked[-check_period:])),2)\n",
    "            if mae_arima_first < minimum_mae:\n",
    "                    minimum_mae = mae_arima_first\n",
    "                    minimum_param_arima = param\n",
    "        except:\n",
    "            continue\n",
    "  moddel_arima = sm.tsa.statespace.SARIMAX(train_data, order = minimum_param_arima, simple_differencing = simple_diff)\n",
    "  result_arima = moddel_arima.fit(disp = False)\n",
    "  predict = result_arima.get_forecast(steps = n_period)\n",
    "  pred_arima = predict.predicted_mean[-n_period:]\n",
    "  predictions_arima_final = pred_arima.mask(pred_arima<0,0).apply(np.ceil)\n",
    "  predictions_arima_final_raw = pred_arima.mask(pred_arima<0,0)\n",
    "  max_inside_history = max(train_data)\n",
    "  if flag == 1:\n",
    "        mae_arima =np.round(np.mean(np.abs(test_data - predictions_arima_final)),2)\n",
    "        return \"ARIMA\", mae_arima\n",
    "  else:     \n",
    "        if resample_type == \"M\":\n",
    "            std_arima = np.std(train_data[-24:])     # predictions_arima_final\n",
    "            if std_arima > 0:\n",
    "                ci_arima = 1.28 * std_arima          ### 80% Confidence Level\n",
    "            else:\n",
    "                ci_arima = (predictions_arima_final * .15)\n",
    "            dict_sheet = {}\n",
    "            conf_int_arima=pd.DataFrame({\n",
    "                                        'lower': np.floor(predictions_arima_final - ci_arima),\n",
    "                                        'upper': np.ceil(predictions_arima_final + ci_arima)}, \n",
    "                                         index=predictions_arima_final.index)      \n",
    "            minimum_forecasted_val = conf_int_arima.lower.mask(conf_int_arima.lower<0,0)\n",
    "            maximum_forecasted_val = conf_int_arima.upper.mask(conf_int_arima.upper<0,0)\n",
    "            weight_list = [3,3,4,5,5,7,8,10,10,15,15,15] #= 100\n",
    "            # weight_list = [12,18,20,25,25]\n",
    "            # weight_list = [10,20,35,35]\n",
    "            historic_value = list(train_data[-12:])\n",
    "            weighted_avg = np.round(np.average( historic_value, weights = weight_list), 2)\n",
    "            weighted_avg = np.maximum(np.ceil(weighted_avg), 0)     #np.ceil(weighted_avg) \n",
    "            if minimum_forecasted_val[0] > max_inside_history or np.ceil((.7 * weighted_avg)) > maximum_forecasted_val[0]:\n",
    "                forecasted_avg = np.ceil(weighted_avg)\n",
    "                forecasted_min = np.floor(forecasted_avg - (forecasted_avg * .2))\n",
    "                forecasted_max = np.ceil(forecasted_avg + (forecasted_avg * .2))\n",
    "\n",
    "                predictions_arima_final[predictions_arima_final.index[0]] = forecasted_avg\n",
    "                conf_int_arima.lower[predictions_arima_final.index[0]] = forecasted_min\n",
    "                conf_int_arima.upper[predictions_arima_final.index[0]] = forecasted_max\n",
    "\n",
    "                dict_sheet['Forecasted_value'] = predictions_arima_final.mask(predictions_arima_final<0,0)\n",
    "                dict_sheet['Lower'] = conf_int_arima.lower.mask(conf_int_arima.lower<0,0)\n",
    "                dict_sheet['Upper'] = conf_int_arima.upper.mask(conf_int_arima.upper<0,0)\n",
    "            else:\n",
    "                dict_sheet['Forecasted_value'] = predictions_arima_final.mask(predictions_arima_final<0,0)\n",
    "                dict_sheet['Lower'] = conf_int_arima.lower.mask(conf_int_arima.lower<0,0)\n",
    "                dict_sheet['Upper'] = conf_int_arima.upper.mask(conf_int_arima.upper<0,0)\n",
    "            return dict_sheet\n",
    "        else:\n",
    "            std_arima_raw = np.std(train_data[-24:])    #predictions_arima_final_raw)\n",
    "            if std_arima_raw > 0:\n",
    "                ci_arima_raw = 1.28 * std_arima_raw     ### 80% Confidence Level\n",
    "            else:\n",
    "                ci_arima_raw = (predictions_arima_final_raw * .2)\n",
    "            conf_int_arima=pd.DataFrame({\n",
    "                                      'lower': np.floor(predictions_arima_final_raw - ci_arima_raw),\n",
    "                                      'upper':np.ceil(predictions_arima_final_raw + ci_arima_raw)},\n",
    "                                       index=predictions_arima_final_raw.index)\n",
    "            dict_sheet = {}\n",
    "            dict_sheet['Forecasted_value'] = np.round(predictions_arima_final_raw)\n",
    "            dict_sheet['Lower'] = conf_int_arima.lower.mask(conf_int_arima.lower<0,0)\n",
    "            dict_sheet['Upper'] = conf_int_arima.upper.mask(conf_int_arima.upper<0,0)\n",
    "            return dict_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_method_first_phase(percentage_of_nonzero, product_sales_data):\n",
    "    resample_type = \"M\" #\"M\"\n",
    "    product_len = len(product_sales_data)\n",
    "    thresold_nonzero, test_period = set_all_attribute(resample_type)\n",
    "    if percentage_of_nonzero >= thresold_nonzero:\n",
    "        train_data = product_sales_data[:-test_period]\n",
    "        test_data = product_sales_data[-test_period:]\n",
    "    else:\n",
    "        product_len = train_data = test_data = None\n",
    "    return product_len, train_data, test_data, thresold_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_product_details(product, result):\n",
    "    data_product = result[result['Product_Id'] == product]\n",
    "    final_ = data_product.loc[:,['Date','Quantity']]\n",
    "    final_[\"Date\"]= pd.to_datetime(final_[\"Date\"], dayfirst=True)\n",
    "    final_.set_index('Date', inplace=True)\n",
    "    final_.fillna(0,inplace=True)\n",
    "    final_ =final_['Quantity'].resample('M').sum()\n",
    "\n",
    "    data = [{'Quantity': 0}]\n",
    "    today = datetime.date.today()\n",
    "    end_date = today.replace(day=1) - datetime.timedelta(days=1)\n",
    "    start_date = end_date + relativedelta( years=-5 )\n",
    "    df = pd.DataFrame( data, index=pd.date_range( start=start_date, end=end_date, freq=\"M\" ) )\n",
    "    df.index.rename( 'Date', inplace=True )\n",
    "    ddf = df.squeeze()\n",
    "    final__ = ddf.add( final_, fill_value = 0 )\n",
    "    return final__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_check_final(model_1, mae_1, model_2, mae_2):\n",
    "    if mae_1 < mae_2:\n",
    "        return model_1\n",
    "    else:\n",
    "        return model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_method_based_length( train_data, test_data, baseline_value):\n",
    "    flag = 1\n",
    "    test_period = len(test_data)\n",
    "    resample_type = \"M\"\n",
    "    model_1, mae_1 = ARIMA(train_data, test_data, test_period, flag, resample_type, baseline_value)\n",
    "    model_2, mae_2 = Double_Exponential_Smoothing(train_data, test_data, test_period, flag, resample_type)\n",
    "    final_method = method_check_final(model_1, mae_1, model_2, mae_2)\n",
    "    return final_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_function_forecast(train_data, method, check_period):\n",
    "    test_data = 0\n",
    "    flag = 2\n",
    "    if method == \"ARIMA\": \n",
    "        forecast_result = ARIMA(train_data, test_data, 8, flag, \"M\", check_period)\n",
    "    elif method == \"Double Exponential Smooting\":\n",
    "        forecast_result = Double_Exponential_Smoothing(train_data, test_data, 8, flag, \"M\")\n",
    "    return forecast_result, method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sheet_2 = {}\n",
    "for ind, product in enumerate(all_product):\n",
    "    try:\n",
    "        prod_sales_data = lock_product_details(product, result)\n",
    "        non_zero_length = np.count_nonzero(prod_sales_data)\n",
    "        percentage_of_nonzero = (non_zero_length / len(prod_sales_data)) * 100\n",
    "        product_len, train_data, test_data, thresold_nonzero = check_method_first_phase(percentage_of_nonzero, prod_sales_data)\n",
    "        dict_sheet_2[product] = {}\n",
    "        if product_len != None:\n",
    "            # method = check_method_based_length( train_data, test_data, int(len(test_data)) )\n",
    "            # method = method.strip()\n",
    "            method = \"ARIMA\"\n",
    "            forecast_result, method_r = all_function_forecast(prod_sales_data, method, int(len(test_data)))\n",
    "            dict_sheet_2[product]['Historic Data'] = prod_sales_data[-24:].to_string() #-25  [-9:]\n",
    "            dict_sheet_2[product]['Forecasted_Value'] = forecast_result[\"Forecasted_value\"].to_string()\n",
    "            dict_sheet_2[product]['Min Qty'] = forecast_result[\"Lower\"].to_string()\n",
    "            dict_sheet_2[product]['Max Qty'] = forecast_result[\"Upper\"].to_string()\n",
    "            # dict_sheet_2[product]['Method'] = method\n",
    "        # if ind==100:\n",
    "            # print(percentage_of_nonzero, non_zero_length, method)\n",
    "            # break\n",
    "    except:\n",
    "        continue\n",
    "pd.DataFrame.from_dict(dict_sheet_2, orient='index').to_csv('Product_wise_forecast_monthly.csv', index_label=\"Product_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
